---
title: "ML Benchmarks"
params:
  answers: true
  #answers: false
mainfont: Arial
fontsize: 12pt
urlcolor: blue
output: 
  html_document:
    toc: true
    toc_depth: 1
    toc_float: true
    df_print: paged
    theme: paper
    pandoc_args: --output=10_benchmark_answers.html
---

# Introduction

Today, we will learn how to use conduct a ML benchmark experiment in `R` using the `mlr3` ecosystem.

In this practical, we will use a data set (`drugs.csv`) containing information on personality profiles of drug users and non-users. It is a pre-processed data set to facilitate its usage in this exercise. The original data set can be retrieved from the UCI Machine Learning Repository (you can find the data [here](https://archive.ics.uci.edu/ml/machine-learning-databases/00373/drug_consumption.data)).

The following variables are available in the data set:

- `Age`: Age of participant (standardized)
- `Gender`: Gender (standardized: -0.48246 = male, 0.48246 = female)
- `Nscore`: Standardized neuroticism score
- `Escore`: Standardized extraversion score
- `Oscore`: Standardized openness score
- `Ascore`: Standardized agreeableness score
- `Cscore`: Standardized conscientiousness score
- `Impulsivity`: Standardized impulsivity score
- `SS`: Standardized sensation seaking score
- `User`: Indicator whether participant is a user/non-user

The goal is to predict whether someone is a user of non-user based on all other variables using the `mlr3verse`. Further information about the functionalities of `mlr3` can be found at https://mlr3book.mlr-org.com/. 

```{r packages, warning = FALSE, message = FALSE}
library(mlr3verse)
library(tidyverse)
library(ggplot2)
library(psych)
```

```{r, make-data, include = F}
data <- read.csv2("data/drugs.csv")
```

---

__1. Get an impression of the data by looking at the structure of the data and creating some descriptive statistics to get an idea of which covariates may be indicative of drug usage.__

```{r explore-data, include = params$answers}
head(data)
tail(data)

data %>%
  select(-c(Gender, User)) %>%
  describeBy(data$User, fast = TRUE)
```

---

__2. To further explore the data we work with, create some interesting data visualizations that show whether there are interesting patterns in the data.__

*Hint:* Think about adding a color aesthetic for the variable `User`.

```{r exploratory-viz, include = params$answers, cache = TRUE}
data %>%
  select(-Gender) %>%
  pivot_longer(where(is.numeric)) %>%
  ggplot(aes(x = value, col = User, fill = User)) +
  geom_boxplot(alpha = 0.8) +
  facet_wrap(~name, scales = "free") +
  scale_color_brewer(palette = "Paired") +
  scale_fill_brewer(palette = "Paired") +
  theme_minimal()

prop.table(table(data$Gender, data$User), margin = 1) %>%
  as.data.frame %>%
  select(Gender = Var1, User = Var2, `Relative Frequency` = Freq) %>%
  ggplot(aes(y = `Relative Frequency`, x = Gender, col = User, fill = User)) +
  geom_histogram(alpha = 0.8, stat = "identity", position = "dodge") +
  scale_fill_brewer(palette = "Paired") +
  scale_color_brewer(palette = "Paired") +
  theme_minimal()

## info: -0.48246 = male, 0.48246 = female
## Percentage of males using/ having used drugs appears to be much higher than for females
```

---

__4. Create a classification task in mlr3 defining the objective for the ML benchmark.__

```{r task, include = params$answers}
# transform criterion to factor variable (mlr3 expects a factor variable when a classification task is created ):

data$User <- as.factor(data$User)

task <- TaskClassif$new(id = "drugs-data", # arbitrary name (important if benchmark uses multiple data sets)
                        backend = data, # data set
                        target = "User", # target variable
                        positive = "User") # label considered "positive" class
```

---

__5. Set up a complex nested resampling strategy by defining an inner and an outer resampling. The inner resampling should be a simple train-test split (70-30). Choose a four-fold CV for the outer resampling__

```{r resampling, include = params$answers}
# we use a relatively simple resampling strategy to save computation time
# in a real benchmark, you may want to use more folds and/or repeated CV for the outer resampling and CV for the inner resampling
res_outer <- rsmp("cv", folds = 4)
res_inner <- rsmp("holdout", ratio = 0.7)
```

---

__6. We want to tune some hyperparameters of two ML algorithms. Define a tuning scheme (i.e., a tuner and a terminator). For simplicity (and to save computation time), use random search with 200 iterations. Define a performance measure that should be optimized during hyperparamter tuning. Using the accuracy as a performance measure makes sense as most algorithms are trained to optimize the accuracy/MMCE (you can choose a different measure though, for example, if you want to focus more on one of the possible classification errors).`__

```{r tuner, include = params$answers}
# do not use too many iterations (n_evals) here to keep the runtime low
terminator <- trm("evals", n_evals = 200)
tuner <- tnr("random_search")

# performance measure that is later used for tuning
mes_acc <- msr("classif.acc")
```

---

__7. Set up a list of learners that should be compared in this benchmark. Use a baseline ("featureless") learner, a logistic regression model, a random forest (use the ranger implementation as it is the fastest), an SVM, and an XGBoost (gradient boosting algorithm).__

*Note.* Set `predict_type = "prob"` for each algorithm to obtain predicted probabilities which allows us to calculate a broader range of evaluation metrics.

```{r learners, include = params$answers}
baseline <- lrn("classif.featureless",  predict_type = "prob")
logreg <- lrn("classif.log_reg", predict_type = "prob")
ranger <- lrn("classif.ranger", predict_type = "prob")
svm <- lrn("classif.svm", predict_type ="prob", type = "C-classification")
xgboost <- lrn("classif.xgboost", predict_type = "prob")
```

---

__8. We now want to tune hyperparameters of the XGBoost and the SVM. Define the parameter set that should be tuned and set up the respective modeling pipeline using the AutoTuner function. For the XGBoost, we want to tune the number of trees (nrounds) between 1 and 500, the learning rate (eta) between 0.001 and 0.2, and the maximum tree depth between 1 and 10. For the SVM, we want to tune the kernel function (i.e., choose between radial and linear) and the cost parameter between 0 and 50.__

```{r autotune, include = params$answers}
# this is just an example using three of the most important hyperparameters of the xgboost algorithm, this is probably not sufficient to reach the best performance possible
param_set_xgb <- ps(
  nrounds = p_int(lower = 1, upper = 500),
  eta = p_dbl(lower = 0.001, upper = 0.2),
  max_depth = p_int(lower = 1, upper = 10)
)

# Sometimes you have to load the mlr3tuning package separately (if you receive an error message stating that R cannot find an object called 'AutoTuner')
# library(mlr3tuning)

xgb_tuned <- AutoTuner$new(
  learner = xgboost, # basic xgboost learner (see above)
  resampling = res_inner, # inner resampling to evaluate different param sets (defined above)
  measure = mes_acc, # performance measure to select parameters (see above)
  search_space = param_set_xgb, # parameter range that is tested
  terminator = terminator, # termination criterion
  tuner = tuner) # tuning scheme (here: random search)

## SVM (this also only an example; to optimize the performance in a real application a more extensive tuning might be necessary)

param_set_svm <- ps(
  cost = p_dbl(lower = 0, upper = 50),
  kernel = p_fct(c("radial", "linear"))
)

svm_tuned <- AutoTuner$new(
  learner = svm, # basic svm learner (see above)
  resampling = res_inner, # inner resampling to evaluate different param sets (defined above)
  measure = mes_acc, # performance measure to select parameters (see above)
  search_space = param_set_svm, # parameter range that is tested
  terminator = terminator, # termination criterion
  tuner = tuner) # tuning scheme (here: random search)
```

---

__9. To exemplify how to integrate a complex modeling pipeline in a benchmark, we want test how a logistic regression model with variable selection performs in comparison to the logistic regression model that uses the full feature set. Therefore, create a filter that selects the two most relevant features based on the training data and create a so-called graph learner to include the complete modeling pipeline in the resampling procedure.__

*Note*: Use the F-statistic of an ANOVA to select the features that are most strongly related to the outcome. You can simply create a filter with the `flt()`-function (https://mlr3filters.mlr-org.com/reference/mlr_filters.html).

```{r select, include = params$answers}
filter = flt("anova")
filter = po("filter", filter = filter)
filter$param_set$values$filter.nfeat = 2
logreg_varselect = GraphLearner$new(filter %>>% logreg)
```
---

__10. Set up and run the benchmark experiment. Make sure to parallelize the execution to save computation time.__

*Note*: Set a seed to make the results reproducible!

```{r benchmark, include = params$answers}
# Parallelize the execution using the future package
# adjust the number of workers depending on your machine
future::plan("multisession", workers = 6)

set.seed(40123142)


grid <- benchmark_grid(tasks = list(task),
                       # include all learners (baseline, logreg, logreg with var selection, tuned xgboost, and tuned svm):
                       learners = list(baseline, logreg, logreg_varselect, ranger, xgb_tuned, svm_tuned),
                       resamplings = list(res_outer)
)

results <- benchmark(grid, store_models = FALSE) # store_models = FALSE to save memory

future::plan("sequential")
```

---

__11. Now aggregate the results using different performance metrics - accuracy, sensitivity, specificty and auc. Also make sure to calculate not only the mean over the four folds but also the standard deviation to get an idea of how stable the performance estimates are.__

```{r eval, include = params$answers}
mes_list <- list(
  msr("classif.sensitivity"),
  msr("classif.sensitivity", id = "classif.sensitivity.sd", aggregator = sd),
  msr("classif.specificity"),
  msr("classif.specificity", id = "classif.specificity.sd", aggregator = sd),
  msr("classif.acc"),
  msr("classif.acc", id = "classif.acc.sd", aggregator = sd),
  msr("classif.auc"),
  msr("classif.auc", id = "classif.auc.sd", aggregator = sd)
)

results$aggregate(mes_list)
```

---

__12. Use the autoplot function from the `mlr3viz` package to create a boxplot to visualize the accuracy of each learner in the benchmark experiment.__

```{r viz, include = params$answers}
# autoplot can be used to create a nice plot that summarizes the benchmark results (e.g., a boxplot)
mlr3viz::autoplot(results, measure = msr("classif.acc"), type = "boxplot")
```

---

# Hand-in

When you have finished the practical, 

- enclose all files of the project `10_benchmark.Rproj` (i.e. all `.R` and/or `.Rmd` files including the one with your answers, and the `.Rproj` file) in a zip file, and 

- hand in the zip [here](). 

---
